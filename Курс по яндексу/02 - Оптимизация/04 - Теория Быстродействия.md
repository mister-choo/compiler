# Всегда ли нужно измерять

В прошлой теме вам удалось невероятное: ускорить операцию оборачивания вектора в тысячи раз, просто изменив место добавления элементов. С точки зрения написания кода нет большой разницы, куда добавлять элемент — в начало или в конец, а с точки зрения производительности разница огромная. Но возможно, есть способ и без профилировщика понять, что вставлять нужно именно в конец. Тогда удастся сразу реализовать быстрый вариант.

Иногда действительно можно заранее оценить сложность алгоритма и не тратить время на написание заведомо неприемлемой по скорости реализации. Для примера посчитаем количество операций, которые совершит программа при оборачивании вектора в двух случаях.

**Первый случай:** при вставке в конец.

**Второй случай:** при вставке в начало.

Будем считать только операции определённого вида — запись одного числа в память компьютера. Также будем считать, что мы заранее зарезервировали место в векторе методом `reserve`, так что вставки в конец требуют только одной операции.

Вставка в начало вектора размера $K$ потребует $K + 1$ операцию, так как нужно переместить KK существующих элементов вектора и записать ещё одно число. Мы посчитали одно перемещение за одну операцию, потому что переместить — это прочитать и записать в другое место, а чтения в данном примере не считаются. Вставка в конец требует только одну операцию записи.

Теперь представим, что вставляем не один раз, а много — обозначим это количество вставок через NN.

**Первый случай:** делаем $N$ раз по одной операции. В сумме получим $N$.

**Второй случай:** в первый раз вектор пуст, будет одна операция записи. Во второй раз уже две операции, в третий — три. Просуммируем все количества операций для каждой из $N$ вставок и получим сложное выражение: $1 + 2 + \cdots + N$ . Можно без труда вычислить по формуле суммы арифметической прогрессии. Ответом будет $\cfrac{N^2+N}2$​.

Теперь предположим, что размер вектора 10 000 элементов. Вычислим общее количество операций.

**Первый случай:** 10 000 операций.

**Второй случай:** 50 005 000 операций.

Разница очевидна! Запуская программу, вы оценили это на себе, а вернее — на оборудовании своего компьютера. Причина такого серьёзного отличия в степени 2. Она есть в правом выражении, а в левом её нет.

Наглядно покажет разницу $N$ и$ \cfrac{N^2 + N}2$ такой график:

![image](https://pictures.s3.yandex.net/resources/Group_50_1636222040.png)_Сравнение величин при N до 10_

![image](https://pictures.s3.yandex.net/resources/Group_51_1636222088.png)_Сравнение величин при N до 100_

Алгоритм, в котором встречается $N^2$, возникает и в обычных жизненных ситуациях. Представьте большой офис, где работают 50 сотрудников. Приходя на работу, они здороваются с каждым за руку.

# От константы к квадрату

В прошлом уроке вы много считали. И считали не зря: вы увидели, что все полученные результаты разбились на два лагеря.

В первом лагере в ответе было NN, но не было N^2N2, и числа получались не такими большими. Выражения, где есть NN в степени не выше единицы, называются линейными. Примеры линейных выражений: NN, 2N2N, 4N + 64N+6, 0.5N-1000.5N−100. Для этих выражений есть специальное обозначение: O(N)O(N) — произносится «о от эн».

Во втором лагере вы столкнулись с большими числами: 1225, 5050 и, наконец, 50 005 000. Всё потому, что в выражениях, из которых получились эти числа, фигурировал пресловутый N^2N2. Это квадратичная зависимость — есть NN во второй степени, но нет NN в большей. Приведём примеры и тут: $\cfrac{N^2+N}2$, 1$0N^2 + 5N +1$, $\cfrac {N^2}{100} + 10$. Всё это можно обозначить символом $O(N^2)$ — произносится «о от эн квадрат».

Правило простое: из выражения убирают всё, кроме самого важного — в рассмотренных примерах это NN или N^2N2. Эти выражения покажут не сухой остаток точного количества операций, а то, что называется скоростью роста функции, или асимптотикой. Посмотрим, как это применить в программировании.

> Первое правило асимптотики: убираем всё меньшее, оставляем главное.

Считать точное количество операций, которые выполнит программа, бывает непросто. А главное, это мало что даёт, ведь процессор выполняет больше операций, чем мы можем представить, глядя на код. Но это ещё полбеды. Разные операции имеют разную продолжительность. Бывает, что одна и та же операция иногда выполняется быстро, а иногда медленно. Всё это говорит о том, что абстрактное понятие «количество операций» или «количество сложений», «количество сравнений» не имеет большого смысла.

Допустим, программа A делает $0.5N^2$ операций, а программа B делает $1000\cdot N + 10000$ операций, причём операции программы B в 100 раз медленнее, чем операции программы A. Какая программа быстрее завершит работу при $N = 1\ 000\ 000$?

Программа B в несколько раз быстрее.
Именно! В то время как программа A делает 500 тысяч миллионов операций, программа B сделает «всего» 100 тысяч миллионов и ещё один миллион в эквиваленте операций А. То есть B примерно в пять раз быстрее.


Как видите, при большом $N$ важен не коэффициент 1000 и длительность одной операции, а именно асимптотика. Если количество операций в A было $O(N^2$), то у B это $O(N)$, благодаря чему при большом $N$ программа B оказалась быстрее. Важно, что при увеличении $N$ разница будет существенно расти: например, при $N=10\ 000\ 000$ она составит уже примерно 50 раз!

Эти графики показывают: при увеличении $N$ разница между $N$ и $N^2$ возрастает, а все коэффициенты, добавки и прочие постоянные факторы остаются неизменными.

![image](https://pictures.s3.yandex.net/resources/Group_52_1636222234.png)

---

В программировании важно, как программа поведёт себя при больших объёмах входных данных. Когда их мало, она и так, скорее всего, выполнится быстро. Поэтому главный вопрос — как будет расти количество операций программы, когда её параметры становятся большими. Чтобы ответить на этот вопрос, вводится понятие скорости роста количества операций — асимптотики. Оно обозначается буквой $O$. Асимптотика — это скорость роста. Количество, выражаемое формулой $O(N)$, может при некотором $N$ быть даже больше, чем другое количество, выражаемое формулой $O(N^2)$ Но скорость роста у $O(N)$ меньше. Поэтому при увеличении $N$ неизбежно настанет момент, когда $O(N^2)$  станет больше. На практике этот момент обычно настаёт быстро. А далее разница становится уже колоссальной, как на графике.

Самый простой, но важный пример асимптотики — константа. Она обозначается $O(1)$ и показывает, что оцениваемое количество известно заранее. Вернее, оно может варьироваться, но никогда не будет превышать заданное число, не зависящее ни от каких входных данных. Данных может быть очень и очень много, а оцениваемое количество всё равно останется в определённых пределах.

Например, Вася, приходя на работу, жмёт руку каждому из $N$ коллег. Он делает $N-1$, то есть $O(N)$ рукопожатий. А более замкнутый Олег персонально здоровается только со своими друзьями: Васей, Петей и Колей. Количество его рукопожатий будет всего лишь $O(1)$.

# Оцениваем сложность программы

Считать рукопожатия интересно, но мы занимаемся программированием, поэтому будем считать и оценивать количество операций. Сначала нужно понять, от чего оно зависит. Для многих функций количество операций уже оценено до нас, и эту оценку можно найти в документации. В уроке вы научитесь это делать.

Асимптотику количества операций программы называют асимптотической сложностью, но мы будем говорить просто «сложность программы».

Количество операций зависит от различных факторов. Прежде всего определитесь, что именно измерять: отдельную функцию, алгоритм, уже реализованный в C++, или всю программу целиком. Затем подумайте, от каких данных и параметров зависит количество операций, и как их измерить.

Параметром может быть число, которое вводит пользователь, или размер массива, переданного функции. Если речь идёт об алгоритме, выполняющем работу на участке контейнера между двумя итераторами, важной мерой будет расстояние между этими итераторами — количество шагов от начального до конечного. В операциях с `vector`, `deque`, `map`, `set` часто имеет значение количество элементов, уже находящихся в контейнере, или расстояние от элемента, над которым производится операция, до конца контейнера. Операция с одним элементом контейнера нередко выполняется за $O(1)$ операций.

Узнать сложность стандартного алгоритма C++ или метода контейнера можно из документации на сайте [cppreference.com](https://cppreference.com/).

Мы рассматривали выражения, которые сводились к $O(N)$, $O(N^2)$. Однако в реальности количество операций может зависеть не от одной меры, а от разных. Выражения могут усложняться, но вычислять через $O$ всё равно гораздо проще, чем напрямую.

Вспомните первое правило асимптотики — убрать всё лишнее, оставить под $O$ только главное. Теперь вы готовы к тому, чтобы узнать второе:

> Второе правило асимптотики: оценивать худший случай.

При вставке в начало вектора количество операций варьировалось от 1 до N. Итоговую сложность мы оценили как O(N^2)O(N2). Если бы мы сразу рассмотрели худший случай, когда в векторе уже NN элементов, можно было оценить сложность одной вставки как O(N)O(N). Тогда для вычисления не пришлось бы даже вспоминать формулу прогрессии: мы сделали NN раз по O(N)O(N) операций и получили O(N^2)O(N2). А это правильный ответ.

Если бы мы были оптимистами и оценивали лучший случай — одна операция при вставке — полученная оценка O(N)O(N) не отражала бы реальность. Учитывая одну только сложность, определить разницу между вставками в начало и в конец не вышло бы.

>📖 С худшим случаем тоже надо быть осторожным, особенно когда операция выполняется многократно. Получится правильная верхняя граница, но она в некоторых случаях слишком груба. Подробно изучим это в уроке про амортизированную сложность.

Иногда оценка худшего случая — это не выбор, а необходимость. Например, когда сложность какой-либо операции неизвестна. Перемешаем числа 1, 2, ..., r случайным образом и поищем число r/2:


```cpp

#include <algorithm>
#include <iostream>
#include <numeric>
#include <vector>
#include <random>

using namespace std;

int main() {
    int r;
    cin >> r;

    vector<int> v(r);

    // 1. заполним числами от 1 до r и 
    iota(v.begin(), v.end(), 1);

    // 2. перемешаем их случайным образом
    random_shuffle(v.begin(), v.end());

    // 3. ищем число r / 2
    int pos = find(v.begin(), v.end(), r / 2) - v.begin();
    cout << r / 2 << " находится на позиции "s << pos << endl;
} 

```

Оценка худшего случая — это не каприз. Есть минимум три причины выбирать именно худший случай:

1.  Так проще оценивать. Например, в нашем случае невозможно предсказать, сколько точно шагов сделает `find`.
2.  Нежелательно, когда программа обычно работает быстро, но при случайном стечении обстоятельств вдруг подвисает.
3.  Программа с плохой сложностью может подвисать не случайно, а при специально подобранных входных данных. Этим могут воспользоваться злоумышленники.

Иногда оценивают среднюю сложность — усреднённое количество операций, которое делает программа. Эта оценка, как правило, труднее, но именно её используют в некоторых алгоритмах.

>📖 Однажды на сайте Stack Overflow произошёл сбой. Главная страница и ряд других упорно отказывались грузиться. Дело было в алгоритме, который имел плохую сложность относительно конкретного параметра — количества идущих подряд пробелов в тексте заданного вопроса. Пользователь разместил вопрос, содержащий несколько тысяч пробелов подряд, и этим обрушил сайт.

Мы оценили не все операции. В программе есть ввод из `cin`, инициализация переменных, вывод в `cout`, не говоря уже о том, что сама программа при запуске требует инициализации операционной системой. Но все эти операции — константные, они не зависят от размера введённого числа `r`. Зависеть могут ввод и вывод, а количество цифр числа `int` никогда не превышает 11. По первому правилу оценки оставляем только главное — три алгоритма, имеющие сложность $O(r)$. Нетрудно понять, что все они в сумме будут эквивалентны одному $O(r)$. Подробнее это разберём в уроке об арифметике сложности.

# Улучшаем сложность

В предыдущих уроках было много слов, но мало дела. Исправим это скорее. Начнём с примера. В конце урока вы ускорите его в десятки раз.

Даны два вектора чисел. Нужно определить, содержат ли они в точности одинаковые элементы. Элементы в векторах могут быть в разном порядке.

Решение:
```cpp



#include <algorithm>
#include <vector>

using namespace std;

// функция возвращает true, если векторы из одинаковых элементов
bool TestPermut(const vector<int>& v1, const vector<int>& v2) {
    // если они разной длины, элементы заведомо разные
    if (v1.size() != v2.size()) {
        return false;
    }

    for (int i : v1) {
        // проверяем, что каждый элемент первого вектора
        // содержится одинаковое количество раз в обоих векторах
        if (count(v1.begin(), v1.end(), i) != count(v2.begin(), v2.end(), i)) {
            return false;
        }
    }

    return true;
}
```
 


Измерим время работы цикла. Он совершает $N$ итераций. Каждая итерация два раза вызывает алгоритм `count`. На его [странице](https://en.cppreference.com/w/cpp/algorithm/count) на сайте cppreference.com сказано, что его сложность в худшем случае — это линейное количество константных сравнений относительно расстояния между итераторами, то есть $O(N)$. Сделать $N$ раз $O(N)$ — это $O(N^2)$.

---

Такие сложности есть у массы методов и алгоритмов в стандартной библиотеке. Вы можете самостоятельно убедиться в этом, изучая их страницы на cppreference.com.

Часто сложность можно определить интуитивно, представив, как именно работает функция. Приведём несколько примеров. Постарайтесь в каждом из них понять, почему сложность именно такая.

В частности, $O(1)$— это сложность

-   получения или изменения элемента `vector`, `deque`, `string` по итератору;
-   получения или изменения элемента `vector` и `string` по номеру;
-   перемещения итератора `vector` или `deque` на произвольное число в любую сторону;
-   перемещение итератора `map` или `set` на один элемент;
-   вставки в `stack`, в конец `vector`, в начало или конец `deque`, в любое место `list` по итератору. Для вектора эта сложность амортизированная. Что это значит, вы узнаете в одном из следующих уроков;
-   удаления в аналогичных случаях.

В одной из прошлых тем вы реализовали стек, который выдаёт наименьший элемент. Сложность получения элемента из такого стека константная — $O(1)$.

$O(N$) — это сложность

-   любых алгоритмов, которые один раз проходят от одного итератора до другого: `find`, `find_if`, `iota`, `count`, `count_if`, `transform`;
-   копирования любого контейнера или строки: `vector`, `set`, `map`, `deque`, `stack`;
-   деструктора любого контейнера;
-   сравнение контейнеров, строк;
-   вставки в начало или середину `vector`, середину `deque`;
-   удаления в аналогичных случаях.

Для методов контейнера $N$ — это количество элементов контейнера, для алгоритмов — расстояние между итераторами.

Все указанные сложности верны, если такие операции с элементами контейнера как удаление, копирование и сравнение константны, то есть имеют сложность $O(1)$. Иначе нужна другая оценка. Например, если есть вектор строк длины не более $L$, сравнение таких строк требует в худшем случае $O(L)$операций. Сравнение векторов размера $N$ из таких строк будет иметь сложность $O(NL)$.

Пусть вектор чисел `v` имеет размер `N`. Также переменная `s` типа `string` содержит `N` символов. Укажите сложность следующих операций:

`vector<int> u = v`

$O(N)$

`v.insert(15, v.begin() + N/2)`

$O(N)$

`v[N/2]=100`

$O(1)$

`v.push_back(s[N/2])`

$O(1)$

`std::vector<string> sv(N, s)`

$O(N²)$



Итак, вы увидели три сложности: $O(1)$, $O(N)$, $O(N^2)$. Далее в уроках вы познакомитесь с очень большой (экспоненциальной) и очень маленькой (логарифмической) сложностями.

Если не помните логарифмы — не пугайтесь. Уроки будут понятными для тех, кто забыл логарифмы, и интересными для тех, кто не забыл. В рамках курса будем считать, что логарифм — это просто количество цифр числа. Например,$\log 99999\approx5$ l. Такое внимание к логарифмам неслучайно. Они фигурируют в важном классе сложности — $O(N\log N$). Эта сложность занимает промежуточное положение между линейной и квадратичной сложностью: хуже, чем $O(N)$, и намного лучше, чем $O(N^2)$:

![image](https://pictures.s3.yandex.net/resources/Group_53_1636222370.png)_Сравнение трёх асимптотик при N до 10_

![image](https://pictures.s3.yandex.net/resources/Group_54_1636222373.png)_Сравнение трёх асимптотик при N до 100_

![image](https://pictures.s3.yandex.net/resources/Group_55_1636222375.png)_Сравнение трёх асимптотик при N до 1000_

Такую сложность имеет эффективная сортировка контейнера из NN элементов. Убедиться в этом можно на [странице алгоритма sort](https://en.cppreference.com/w/cpp/algorithm/sort) в документации.$O(N\log N)$ — это теоретический минимум сложности для сортировки. Невозможно разработать алгоритм сортировки, который бы делал меньше, чем $O(N\log N)$ операций. Вернее, меньше, чем $O(N\log N)$ сравнений.


# Логарифмическая сложность

Рассмотрим такую задачу. В восьмиэтажном доме поселилась невезучая обезьяна, которая очень любит орехи. Орехи она находит неподалёку. Только вот беда — ей совершенно нечем их колоть. Обезьяна хоть и невезучая, но хитрая. Она решила сбрасывать орехи из окна и таким образом разбивать их. Обезьяна успешно расколола орех, кинув его с верхнего, восьмого этажа. Чтобы не подниматься каждый раз так высоко, обезьяна хочет найти самый низкий этаж, при броске с которого орехи разбиваются. Все орехи одинаковые и все броски одинаковые, так что любые два броска с одного этажа будут иметь одинаковый результат.

Подумайте, как действовать обезьяне, чтобы при самом страшном невезении совершить как можно меньше бросков для определения нужного этажа. Сколько бросков ей нужно сделать?

Придётся перебирать все этажи — итого восемь бросков.

Один она уже попробовала, поэтому хватит семи попыток.

Нужно перебрать каждый второй этаж, итого четыре попытки.

Есть способ справиться за три броска.

Один бросок, на этот раз ей точно повезёт!

Совершая бросок, например, с пятого этажа, обезьяна делит этажи на две части. Если бросок успешный, нужный этаж — пятый или ниже. Если неуспешный, оптимальный этаж точно выше — шестой, седьмой или восьмой. Поскольку обезьяна невезучая, скорее всего, каждый раз будет выпадать большая часть: в этом конкретном примере — с первого по пятый. Лучший способ — делать части равными, бросая со среднего этажа. Тогда количество вариантов будет каждый раз сокращаться вдвое: после первого броска обезьяна будет рассматривать четыре этажа, после второго — два, после третьего найдёт нужный.

Число восемь выбрано для условия задачи отнюдь не случайно: чтобы обезьяна могла постоянно делить на два. Также подойдёт любая степень двойки: 2, 4, 8, 16, 32.

---

Если удвоить количество этажей, обезьяне понадобится всего на один бросок больше. Предложенное решение — пример так называемого логарифмического алгоритма, то есть алгоритма сложности $O(\log N)$. Для $N$-этажного здания количество бросков, которое нужно совершить обезьяне, — это двоичный логарифм числа $N$.

Если вы забыли что такое логарифм, не расстраивайтесь — для наших целей можно считать, что это просто количество цифр в числе. А если не забыли, возможно, задались вопросом, почему мы не написали основание. На самом деле под буквой $O$ основание неважно: логарифмы по всем основаниям имеют одинаковую скорость роста.

У логарифмической зависимости есть особенность: если увеличить аргумент $N$ **в несколько раз**, количество действий увеличится **на несколько раз**. Например, увеличили количество этажей в два раза, а количество необходимых бросков возросло на один. Или увеличили число в десять раз, а количество цифр увеличилось на одну. Можно повторить операцию увеличения числа в десять раз шестикратно, и число возрастёт в миллион раз, а количество цифр всего лишь шесть раз увеличится на единицу. Это очень маленький рост.

Важный пример логарифмического алгоритма — бинарный поиск, который использовала хитрая обезьяна. Он реализован в уже известных вам алгоритмах `binary_search`, `lower_bound`, `upper_bound`.

>📖 Поиграйте с кем-нибудь из близких в игру: попросите загадать число от одного до восьми и написать его на бумажке. Затем задавайте об этом числе вопросы, подразумевающие ответы «да» или «нет». Например: «У тебя число от одного до четырёх?». Угадайте число за три вопроса. Увеличьте диапазон до 1...16 и количество вопросов до четырёх. Затем до 1...32 и пяти вопросов.

Всего одним вопросом можете удвоить количество вариантов. Неплохо! Так и работает бинарный поиск.

Вы применяете похожий алгоритм, когда ищете слово в словаре. Вряд ли вы просматриваете каждую страницу, начиная с первой, — поиск был бы слишком долгим. Скорее всего, вы открываете словарь в случайном месте и проверяете, раньше или позже находится ваше слово, постепенно сужая круг поиска. Бинарный поиск в контейнере можно применять только если элементы уже упорядочены, как слова в словаре.

Проверим, действительно ли `upper_bound` так эффективен, как подсказывает теория. Создадим отсортированный вектор из миллиона случайных чисел от 0 до 1 000 000 000 и тысячу раз поищем первое число, превышающее 500 000 000:


```cpp

int main() {
    static const int NUMBERS = 1'000'000;
    static const int SEARCHES = 500'000;

    // для случайных чисел используем на этот раз современный механизм,
    // который позволяет генерировать случайные числа различного типа
    // и распределения. Нам понадобятся целые числа int, равномерно
    // распределённые на отрезке 0..1'000'000'000
    mt19937 generator;
    uniform_int_distribution<int> uniform_dist(0, 1'000'000'000);

    vector<int> nums;
    for (int i = 0; i < NUMBERS; ++i) {
        int random_number = uniform_dist(generator);
        nums.push_back(random_number);
    }
    sort(nums.begin(), nums.end());

    ...
} 

```

В первом случае применим для поиска алгоритм `upper_bound`, а во втором — `find_if`. Алгоритм `find_if` просматривает контейнер, пока не найдёт нужное значение, его сложность $O(N)$:



```cpp
...
int result_number;
{
    LOG_DURATION("std::upper_bound"s);
    for (int i = 0; i < SEARCHES; ++i) {
        auto iter = upper_bound(nums.begin(), nums.end(), 500'000'000);
        result_number = *iter;
    }
}
cout << result_number << endl;

{
    LOG_DURATION("std::find_if"s);
    for (int i = 0; i < SEARCHES; ++i) {
        auto iter = find_if(nums.begin(), nums.end(), [](int x) {
            return x > 500'000'000;
        });
        result_number = *iter;
    }
}
cout << result_number << endl;
... 

```

Посмотрим на результат:


```cpp

std::upper_bound: 0 ms
500000367
std::find_if: 623 ms
500000367 

```

Результат получился одинаковый, но `upper_bound` выдала его в сотни раз быстрее. Вот она, сила логарифма!

В стандартной библиотеке логарифмическая сложность встречается часто. Её имеют следующие методы и алгоритмы:

-   вставка одного элемента в `set` и `map` и удаление из них;
-   поиск элемента в `set` и `map`;
-   методы `lower_bound`, `upper_bound` для `set` и `map`;
-   бинарный поиск по отсортированному `vector` или `deque`: `binary_search`, `lower_bound`, `upper_bound`.

Для методов контейнера $N$ — это количество элементов контейнера, а для алгоритмов — расстояние между итераторами.

Логарифмическая сложность так часто встречается в методах `set` и `map`, потому что в них использованы деревья поиска, имеющие логарифмическую глубину. Подробности об этих структурах вы узнаете позже в курсе.

# Опасности экспоненты

Логарифм — отличная вещь. Число может быть большим, например сто миллионов, а цифр в нём будет всего девять. Девять операций лучше, чем сто миллионов. Но бывает логарифм «наоборот»‎, который называется экспонентой. Он далеко не столь приятен.

В прошлом спринте вы решали задачу о числах Фибоначчи. Эти числа F_nFn​ задаются так: $F_0 = 0, F_1 = 1$, а остальные получаются по формуле 
$F_n=F_{n-1}+F_{n-2}$

Ваше решение могло выглядеть так:



```cpp
#include <cstdint>
#include <iostream>
#include <string>

using namespace std;

// числа Фибоначчи довольно быстро растут, используем int64_t
int64_t F(int i) {
    // тут обработаем i == 0 и i == 1
    if (i <= 0) {
        return 0;
    }

    if (i == 1) {
        return 1;
    }

    // рекурсивно вызовем саму функцию F
    return F(i - 1) + F(i - 2);
}

int main() {
    int i;

    while (true) {
        cout << "Введите индекс: "s;
        if (!(cin >> i)) {
            break;
        }

        cout << "Fi = "s << F(i) << endl;
    }
} 

```

Сомнения может вызвать рекурсия. Ей вычисляется `F`. Но глубина рекурсии не превышает `i`. В наших примерах `i` будет в пределах 100, что вполне допустимо. На первый взгляд программа работает неплохо:


```cpp

Введите индекс: 0
Fi = 0
Введите индекс: 1
Fi = 1
Введите индекс: 2
Fi = 1
Введите индекс: 3
Fi = 2
Введите индекс: 4
Fi = 3
Введите индекс: 5
Fi = 5
Введите индекс: 6
Fi = 8
Введите индекс: 10
Fi = 55
Введите индекс: 20
Fi = 6765
Введите индекс: 30
Fi = 832040
Введите индекс: 40
Fi = 102334155 

```

Но перед вычислением 40-го числа была подозрительная пауза. Введём 45. Поначалу показалось, что программа зависла. Но в конце концов она выдала результат:


```cpp

Введите индекс: 45
Fi = 1134903170 

```

У программы такой простой код, но уже явные проблемы с производительностью! Чтобы оценить их, измерим время, внеся изменения в `main`:


```cpp

...

int main() {
    for (int i = 0; i < 100; ++i) {
        int64_t fi;
        {
            LOG_DURATION("Number "s + std::to_string(i));
            fi = F(i);
        }
        cout << fi << endl;
    }
} 

```

Посмотрим, что выдаст программа:


```cpp

Number 0: 0 ms
0
Number 1: 0 ms
1
Number 2: 0 ms
1
...
Number 27: 3 ms
196418
Number 28: 3 ms
317811
Number 29: 7 ms
514229
Number 30: 12 ms
832040
Number 31: 15 ms
1346269
Number 32: 36 ms
2178309
Number 33: 96 ms
3524578
...
Number 43: 4004 ms
433494437
Number 44: 6674 ms
701408733
Number 45: 10793 ms
1134903170
Number 46: 17412 ms
1836311903 

```

Проблемы начались примерно с 27-го числа и начали увеличиваться лавинообразно, когда количество миллисекунд само стало напоминать числа Фибоначчи. Если подумать, сколько операций совершит вызов `F`, можно понять: это вовсе не случайность. `F` вызывает две такие же функции и будет работать примерно столько, сколько обе они вместе взятые — точь-в-точь как в формуле самих чисел. Если так пойдёт дальше, для вычисления 80-го числа понадобится примерно восемь лет.

Количество операций возрастает минимум в два раза каждые два числа. Такая сложность называется экспоненциальной и обозначается O(a^N)O(aN). Как правило, программы, имеющие такую сложность, непригодны для использования, потому что требуют колоссального времени даже при маленьких входных данных.

>📖 С экспоненциальной сложностью связана одна из самых знаменитых проблем информатики — проблема классов $P$ и $NP$. Она заключается в поиске более быстрого, чем экспоненциальный, алгоритма для ряда задач, либо доказательства, что такого алгоритма не существует. В случае положительного решения проблемы, $P$ и $NP$ банковские и криптографические системы окажутся в опасности. Многие алгоритмы шифрования основаны на предположении, что такого алгоритма для $NP$-задач не существует.


---

Это абсолютно неприемлемая сложность, ведь числа Фибоначчи можно вычислить даже без компьютера, простым сложением в столбик. Числа длинные, работа будет трудоёмкой, но понадобится явно меньше времени. Минус нашего алгоритма в том, что он многократно вычисляет одни и те же числа Фибоначчи, вместо того чтобы сделать это один раз и просто запомнить результат. Избавиться от минуса просто — достаточно постоянно помнить два предыдущих числа:


```cpp

int64_t F2(int i) {
    if (i == 0) {
        return 0;
    }

    int64_t prev0 = 0, prev1 = 1;

    for (int t = 1; t < i; ++t) {
        int64_t next = prev0 + prev1;
        prev0 = prev1;
        prev1 = next;
    }

    return prev1;
} 
```


# Арифметика сложности: три правила вычисления

В предыдущих уроках этой темы вы уже оценивали сложность. Обобщим накопленный опыт и сформулируем правила, по которым можно вычислять в более сложных случаях.

## Правило поглощения

Это правило вам уже знакомо. Оно формулируется так: убираем всё ненужное, оставляя главное. Или более подробно: если программа выполняет несколько действий, сложность всей программы будет равна сложности самого медленного действия. Иными словами, при суммировании нескольких сложностей побеждает самая большая.

>📖 Известна мудрость, что стадо бизонов бежит со скоростью самой медленной особи. Так и со сложностью: программа имеет сложность самой медленной операции.

Более того: сложность поглощает сама себя. То есть если несколько слагаемых одинаковы, нужно оставить только одно.

Будьте внимательны: количество слагаемых не должно зависеть ни от каких параметров, оно должно быть известно заранее. Иначе потребуется применить правило умножения, о котором вы узнаете позже.

---
Возьмём, например, функцию, выдающая таблицу умножения для N чисел из вектора:

```cpp
void F(vector<int> v) {
    // избавимся от отрицательных чисел, если таковые имеются
    for (int i = 0; i < v.size(); ++i) {
        if (v[i] < 0) {
            v[i] = -v[i];
        }
    }

    // отсортируем вектор
    sort(v.begin(), v.end());

    // только теперь выведем таблицу
    for (int i = 0; i < v.size(); ++i) {
        for (int j = 0; j < v.size(); ++j) {
            if (j > 0) {
                cout << "\t"s;
            }
            cout << (v[i] * v[j]);
        }
        cout << endl;
    }
} 

```

Функция `F` выполняет три действия:

1.  избавляется от отрицательных чисел — сложность $O(N)$;
2.  сортирует вектор — сложность $O(N\log N)$;
3.  выдаёт саму таблицу — сложность $O(N^2)$.

Итоговая сложность получается по правилу сложения: $O(N)+O(N\log N)+O(N^2) = O(N^2)$.

## Правило суммирования

Бывает, что сложность программы зависит от нескольких параметров. Рассмотрим такую функцию:


```cpp

void SortPair(std::vector<int>& v1, std::vector<int>& v2) {
    sort(v1.begin(), v1.end());
    sort(v2.begin(), v2.end());
} 

```

Пусть размер первого вектора равен NN, а второго — MM. Тогда функция выполняет две операции со сложностями O(N\log N)O(NlogN) и O(M\log M)O(MlogM), но применить правило поглощения тут не получится: мы не знаем, какая из этих двух сложностей окажется больше. Ничего не остаётся, как не мудрствуя лукаво, оставить сумму: O(N\log N+M\log M)O(NlogN+MlogM).

## Правило умножения

Предположим, мы знаем о функции `F(int k)`, что её сложность равна O(k)O(k). Требуется оценить сложность такого кода:


```cpp

bool F(int k);

bool G(int n) {
    for (int i = 0; i < n; ++i) {
        if (F(i)) {
            return true;
        }
    }
    return false;
} 

```

Понадобятся две вещи:

-   сложность тела цикла;
-   асимптотика количества итераций.

Сложность тела цикла оценим по худшему случаю, когда `i == n-1`. Самое сложное — это вызов `F(i)`, который в этом случае имеет сложность $O(n-1)=O(n)$. Количество итераций тоже оценим по худшему случаю: когда все итерации функция `F` возвращала `false`. Тогда будет $O(n)$ итераций.

Правило умножения в данном случае говорит, что нужно перемножить оба результата: сложность тела и количество итераций. В рассмотренном примере они оба равны O(n)O(n), так что получится $O(n\cdot n)=O(n^2)$.

В примере мы видели реализацию тела цикла, но о функции `F` знали только сложность. При использовании алгоритмов бывает наоборот — сам алгоритм скрыт, но известна функция, которую он будет вызывать.

Рассмотрим знакомый `find_if`. На [его странице](https://en.cppreference.com/w/cpp/algorithm/find) сказано, что если N — расстояние между итераторами, переданными этому алгоритму, он совершает не более N проверок. Казалось бы, можно сразу написать сложность — O(N), но это будет корректно только если сложность одной проверки O(1). А если сама проверка сложная — например, поиск символа в строке, — применяют правило умножения:
```cpp

Скопировать кодCPP

`// получить певую строку, содержащую символ 'A', либо ""s если такой строки нет в векторе
const string& FindStringWithA(const vector<string>& v) {
    auto result = find_if(v.begin(), v.end(), [](const string& s){
        return s.find('A') != string::npos;
    });

    // создадим статическую переменную, содержащую пустую строку
    // чтобы иметь возможность возвратить ссылку на неё,
    // если мы ничего не нашли
    static const string dummy;
    return result == v.end() ? dummy : *result;
}` 

```

Чтобы оценить сложность этой функции, нужно знать сложность метода поиска символа в строке — `find`. Но на [странице документации](https://en.cppreference.com/w/cpp/string/basic_string/find) нет раздела "Complexity". Можно предположить, что в случае поиска символа алгоритм будет работать за линейное время относительно длины строки, так как он просматривает все символы по порядку. Если известно, что строки в векторе имеют длину не более LL, а размер вектора NN, ответом будет число O(NL)O(NL) в соответствии с правилом умножения.

Вы познакомились с основными правилами вычисления сложности и умеете оценивать скорость программ, даже не запуская их.

>📖 Сложный случай, выходящий за рамки курса, — динамическое программирование. Большая задача может сводиться к нескольким меньшим, обычно рекурсивно. Вы видели такой пример в задаче о числах Фибоначчи, и смогли посчитать сложность, которая оказалась неприемлемо большой. В общем случае оценка делается сложнее — по основной теореме о рекуррентных соотношениях.

# Амортизированная сложность

Вы уже встречали словосочетание «амортизированная константа» в начале темы. Тогда амортизированной O(1) была названа операция вставки в конец вектора. Далее в курсе вы узнаете, как она устроена, и почему O(1) в этом случае амортизирована. А пока рассмотрим совершенно другой пример.

Одно из прошлых заданий состояло в том, чтобы использовать два стека для реализации очереди. Реализация могла быть такой:
```cpp
#include <cassert>
#include <stack>

using namespace std;

// моделируем очередь с помощью двух стеков
template <class T>
class QueueFromStack {
public:
    void Push(const T& e) {
        stack1_.push(e);
    }

    T Pop() {
        if (stack2_.empty()) {
            while (!stack1_.empty()) {
                stack2_.push(stack1_.top());
                stack1_.pop();
            }
        }

        assert(!stack2_.empty());

        T top = stack2_.top();
        stack2_.pop();
        return top;
    }

private:
    stack<T> stack1_;
    stack<T> stack2_;
};

int main() {
    QueueFromStack<int> q;
    q.Push(1);
    q.Push(2);
    q.Push(3);
    q.Push(4);
    assert(q.Pop() == 1);
    assert(q.Pop() == 2);
    assert(q.Pop() == 3);
    assert(q.Pop() == 4);
} 

```

Новые элементы кладём в первый стек, а когда нужно вынуть, перекладываем все элементы из первого во второй. Этот алгоритм вы изучили в четвёртом спринте.

>📖 У такого алгоритма может быть и практическое применение. Представьте, что на склад приехала машина с большими плоскими пронумерованными коробками. Её разгружают, складывая коробки друг на друга. За коробками приходят клиенты, и отдавать надо по порядковому номеру: то есть первой — нижнюю коробку. Тогда рабочие должны переложить все коробки в другую стопку и отдавать из неё.

Подумаем, будет ли эта реализация очереди эффективной хотя бы с точки зрения сложности. Сомнения вызывает то, что в константной для стека операции `Pop` есть целый цикл. Пусть задача формулируется так: в очередь положили NN элементов и сделали в общей сложности NN операций `Pop`. При этом `Push` и `Pop` могли идти в любом порядке. Нужно оценить асимптотическую сложность этого алгоритма. Как и раньше, будем оценивать именно худшую сложность. Операция `Push` выглядит просто: она делает одну вставку, выполняющуюся за время O(1).

Сколько операций сделает один вызов `Pop` в худшем случае?

Это простая операция, сложность должна быть O(1).

Мы вынимаем элементы в цикле, а их может быть O(N).

Да, в худшем случае именно так.

В цикле для каждого `pop` делается `push`, поэтому результат будет O(N^2).

Худший случай — это когда положили сразу много — примерно N — элементов, а только потом сделали `Pop`. Тогда цикл также должен будет сделать порядка N операций переброски элемента из одного стека в другой. Получается, время работы `Pop` хуже, чем для обычной очереди, а именно O(N).

---

Поскольку всего сделано NN операций `Pop`, сложность всей программы не обрадует: O(N^2).

Взглянем на ситуацию с точки зрения судьбы каждого элемента. В начале элемент попадает в первый стек, затем извлекается из него и попадает во второй. И только потом извлекается окончательно. Непростая судьба, но всё равно константная. Такой подсчёт показывает, что нужно только по две операции `push` и `pop` на каждый элемент. Итоговая сложность получится уже O(N), а не O(N^2), как было вначале.

Похоже, мы слишком увлеклись худшими случаями, и оценка сложности получилась чересчур грубой. А именно оценка сложности цикла в операции `Pop`. Несмотря на то, что в худшем случае этот цикл действительно делает N операций, суммарное количество его итераций составит не более NN на все вызовы `Pop`.

Если усредним N, увидим, что в среднем вызов `Pop` работает за O(1). Это и есть амортизированная сложность. С точки зрения итогового времени работы программы разницы между O(1) и амортизированной O(1) нет. Но у такой сложности есть особенности. Может случиться, что функция много раз подряд работает быстро, и вдруг происходит плохой случай. Функция подвисает, обрабатывая элементы, которые накопились за всё время.

Иными словами, время работы неравномерно. Такое поведение амортизированной константы иногда нежелательно. Например, когда вы программируете компьютерную игру, которая должна моментально реагировать на действия пользователя. Если реакция почти всё время моментальная или даже быстрее, чем нужно, и вдруг игра подвисает, пользователь вряд ли будет доволен. Равномерность скорости критична и для звуковых приложений. При воспроизведении аудио подвисание будет звучать как громкий и неприятный «пш».

Другой пример:


```cpp

#include <iostream>
#include <string>
#include <deque>

using namespace std;

class TrainTerminal {
public:
    void OnPassengerArrive(const string& name) {
        passengers_on_platform_.push_back(name);
    }

    void OnTrainArrive() {
        cout << "Список пассажиров поезда:"s << endl;
        int id = 1;
        while (!passengers_on_platform_.empty()) {
            cout << (id++) << passengers_on_platform_.front() << endl;
            passengers_on_platform_.pop_front();
        }
    }

    bool FindPassenger(const string& name) const {
        auto iter = find(passengers_on_platform_.begin(), passengers_on_platform_.end(), name);
        return iter != passengers_on_platform_.end();
    }

private:
    deque<std::string> passengers_on_platform_;
}; 

```

Этот класс реализует железнодорожную платформу, на которую прибывают электрички и приходят пассажиры. Считается, что есть только один маршрут, поэтому поезд забирает всех пассажиров, которые в данный момент находятся на платформе. По прибытии поезда программа печатает список всех его пассажиров. Обозначим количество пассажиров через NN. Количество поездов в этот день составляет N/100N/100. Служба безопасности 3N3N раз искала подозрительных пассажиров, применяя `FindPassenger`.

Сложность какого из методов этого класса — амортизированная O(1), в то время, как худшая сложность больше?

`OnTrainArrive`, так как цикл не может отправить одного пассажира дважды.

Да, сложность этого метода O(N)O(N), а амортизированная как раз O(1).

Действительно, в неудачном для пассажиров случае метод `OnTrainArrive` отправит их всех или почти всех в одном поезде, заставляя проклинать ненавистный час пик и транспортную компанию. Поэтому худшая сложность метода составит целых O(N)O(N). Но каждый пассажир будет отправлен лишь однажды, поэтому суммарное количество витков цикла внутри `OnTrainArrive` никогда не превысит NN. Раз N/100N/100 вызовов метода заставили его обработать всего NN пассажиров, значит, в среднем один вызов обрабатывал 100100 пассажиров, что тоже вписывается в обозначение O(1)O.

---

Это ещё один замечательный пример амортизированной сложности: каждый вызов функции может обработать множество элементов, но все вызовы в совокупности обработают не больше, чем их было добавлено.

Не путайте амортизированную сложность со средней. Некоторые алгоритмы могут работать быстро или медленно в зависимости от входных данных. Например, сортировка. Обычно она работает за$O(N\log N)$, но если вектор отсортирован изначально, сортировка может определить это за O(N) и завершиться. В этом лучшем случае её сложность будет O(N). Средней сложностью называют усреднение сложности по всем возможным входным данным. Это вовсе не гарантирует быструю работу в каждом случае. Например, злоумышленники могут специально подобрать данные, чтобы «повесить» алгоритм, даже если он имеет хорошую среднюю сложность.

В то же время амортизированная сложность гарантирует быструю работу — вернее, быструю работу программы в итоге, после выполнения многих подобных операций. Амортизированная сложность не гарантирует, что каждый вызов функции или метода будет быстрым. Но если таких вызовов сделать много, плохих случаев окажется мало, и в совокупности количество операций будет всё равно небольшим.


# Не худший случай

Чтобы оценить, сколько примерно времени будет работать ваш алгоритм, ориентируйтесь на правило: ядро процессора выполняет около миллиарда действий в секунду. Оценка грубая, но в некоторых случаях она полезна. Количество действий можно посчитать теоретически. Разберёмся, всегда ли теория верна.

В начале темы мы рассмотрели функцию, которая определяет, одинаковые ли элементы в двух векторах. Вспомним код:


```cpp

// O(n²)
bool TestPermut(const vector<int>& v1, const vector<int>& v2) {
    // если они разной длины, элементы заведомо разные
    if (v1.size() != v2.size()) {
        return false;
    }

    for (int i : v1) {
        // проверяем, что каждый элемент первого вектора
        // содержится одинаковое количество раз в обоих векторах
        if (count(v1.begin(), v1.end(), i) != count(v2.begin(), v2.end(), i)) {
            return false;
        }
    }

    return true;
}

// O(n log n)
bool TestPermut2(const vector<int>& v1, const vector<int>& v2) {
    auto v1_copy = v1;
    auto v2_copy = v2;

    std::sort(v1_copy.begin(), v1_copy.end());
    std::sort(v2_copy.begin(), v2_copy.end());

    return v1_copy == v2_copy;
} 

```

Возможно, вы обратили внимание на странный момент: наивный квадратичный алгоритм из условия быстро завершался при первом запуске, но для ответа `true` ему нужно было много времени. А второй алгоритм всегда работает примерно одинаково быстро. Сравним алгоритмы на сильно различающихся векторах из 500 000 элементов:

Скопировать код

`O(n²): 2 ms
O(n log n): 181 ms` 

Может показаться, что теория обманывает. Сделаем векторы похожими:


```cpp

int main() {

    // ...

    v2 = v1;
    random_shuffle(v2.begin(), v2.end());
    v2[rand() % v2.size()]++;

    // ...
} 

```

Различие только в одном элементе со случайным индексом. Результаты работы:

Скопировать код

`O(n²): 2286 ms
O(n log n): 178 ms` 

На этот раз всё как и должно быть.

$O(N^2)$ будет хуже $O(N\log N)$уже при не очень больших $N$, порядка сотен. А в нашем случае векторы были по 500 000 элементов.

Если векторы существенно отличаются, первый алгоритм быстро найдёт элемент, который есть в первом векторе, но отсутствует во втором. После этого алгоритм сразу завершится. Но для ответа `true` ему понадобится много времени. А второй, быстрый, алгоритм будет в любом случае выполнять две сортировки — в этом смысле он более стабильный.

---

Подобный эффект можно наблюдать и с `upper_bound`. В уроке о логарифме вы увидели, что этот алгоритм сильно выигрывает у `find_if`. Посмотрим, всегда ли. Вернёмся к тому примеру, но теперь вместо 500 000 000 будем искать число, превышающее 100:
```cpp



...
int result_number;
{
    LOG_DURATION("std::upper_bound"s);
    for (int i = 0; i < SEARCHES; ++i) {
        auto iter = upper_bound(nums.begin(), nums.end(), 100);
        result_number = *iter;
    }
}
cout << result_number << endl;

{
    LOG_DURATION("std::find_if"s);
    for (int i = 0; i < SEARCHES; ++i) {
        auto iter = find_if(nums.begin(), nums.end(), [](int x) {
            return x > 100;
        });
        result_number = *iter;
    }
}
cout << result_number << endl;
... 

```

Результат может быть таким:


```cpp

std::upper_bound: 58 ms
352
std::find_if: 0 ms
352 

```

Оба алгоритма выдали одинаковый результат, но `find_if` сделал это гораздо быстрее вопреки теории.

Как вы думаете, почему в этом случае `find_if` оказался быстрее, несмотря на то, что его сложность больше?

Теория гарантирует преимущество меньшей сложности только при больших $N$, здесь оно недостаточно велико.

Сложность `upper_bound` — это $O(\log N)$, а логарифм сложная вещь — алгоритм будет работать то быстро, то медленно.

Сложность `find_if` — это $O(N)$ в худшем случае. Но тут повезло: случай оказался далеко не самым худшим для этого алгоритма.

Проходя вектор, `find_if` быстро находит нужное число, в то время как `upper_bound` в любом случае будет сходиться к нему бинарным поиском. Однако это не значит, что нужно использовать его — в среднем `upper_bound` всё равно гораздо быстрее.